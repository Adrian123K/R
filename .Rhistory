return((x - min(x)) / (max(x) - min(x)))
}
boston_norm <- as.data.frame(lapply(boston, normalize))
summary(boston_norm$MEDV)
summary(boston$MEDV)
hist(boston$MEDV)
dim(boston_norm)
set.seed(1)
s_cnt<-round(0.7*(nrow(boston_norm)))
s_index<-sample(1:nrow(boston_norm), s_cnt, replace=F)
boston_train <- boston_norm[s_index, ]
boston_test <- boston_norm[-s_index, ]
head(boston_train)
library(neuralnet)
boston_model <- neuralnet(MEDV~CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT, data=boston_train, hidden=10)
model_results <- compute(boston_model, boston_test[1:13])
predicted_houseprice <- model_results$net.result
cor(predicted_houseprice, boston_test$MEDV)
boston_model2 <- neuralnet(MEDV~CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT,
data=boston_train, hidden=c(10, 5), rep=10)
model_results2 <- compute(boston_model2, boston_test[1:13])
predicted_houseprice2 <- model_results2$net.result
cor(predicted_houseprice2, boston_test$MEDV)
sys.setlocale("LC_COLLATE", "ko_KR.UTF-8")
setwd("d:/R")
credit <- read.csv("credit.csv",  stringsAsFactors = TRUE)
str(credit)
prop.table( table(credit$default)  )
hist( credit$amount, col="grey", density=80)
summary( credit$amount)
set.seed(0)
credit_shuffle <-  credit[ sample( nrow(credit) ),  ]
train_num <- round( 0.9 * nrow(credit_shuffle), 0)
credit_train <- credit_shuffle[1:train_num ,  ]
credit_test  <- credit_shuffle[(train_num+1) : nrow(credit_shuffle),  ]
library(C50)
credit_model <- C5.0( credit_train[ ,-17] , credit_train[  , 17] )
credit_result <-  predict( credit_model, credit_test[  , -17] )
library(gmodels)
CrossTable(  credit_test[   , 17], credit_result )
library(ipred)
set.seed(300)
mybag <- bagging(default ~ ., data = credit, nbagg = 25)
credit_pred <- predict(mybag, credit)
table(credit_pred, credit$default)
library(caret)
set.seed(300)
ctrl <- trainControl(method = "cv", number = 10)
train(default ~ ., data = credit, method = "treebag",  trControl = ctrl)
Sys.setlocale("LC_COLLATE", "ko_KR.UTF-8")
setwd("d:/R")
boston<-read.csv("boston.csv", stringsAsFactors=T)
str(boston)
head(boston)
summary(boston$MEDV)
hist(boston$MEDV)
boston<-read.csv("boston.csv", stringsAsFactors=T)
str(boston)
head(boston)
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
boston_norm <- as.data.frame(lapply(boston, normalize))
summary(boston_norm$MEDV)
summary(boston$MEDV)
hist(boston$MEDV)
dim(boston_norm)
set.seed(1)
s_cnt<-round(0.7*(nrow(boston_norm)))
s_index<-sample(1:nrow(boston_norm), s_cnt, replace=F)
boston_train <- boston_norm[s_index, ]
boston_test <- boston_norm[-s_index, ]
head(boston_train)
library(neuralnet)
boston_model <- neuralnet(MEDV~CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT, data=boston_train, hidden=10)
model_results <- compute(boston_model, boston_test[1:13])
predicted_houseprice <- model_results$net.result
cor(predicted_houseprice, boston_test$MEDV)
boston_model2 <- neuralnet(MEDV~CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT,
data=boston_train, hidden=c(10, 5), rep=10)
model_results2 <- compute(boston_model2, boston_test[1:13])
predicted_houseprice2 <- model_results2$net.result
cor(predicted_houseprice2, boston_test$MEDV)
library(devtools)
library(tidyverse)
install.packages('tidyverse')
library(tidyverse)
library(datarium)
install.packages('datarium')
library(datarium)
data("marketing", package = "datarium")
head(marketing, 4)
model <- lm(sales ~ youtube + facebook + newspaper, data = marketing) summary(model)
model <- lm(sales ~ youtube + facebook + newspaper, data = marketing)
summary(model)
x <- data.frame(beer=c(0,1,1,1,0),
bread=c(1,1,0,1,1),
cola=c(0,0,1,0,1),
diapers=c(0,1,1,1,1),
eggs=c(0,1,0,0,0),
milk=c(1,0,1,1,1) )
x
# 2. arules 패키지를 설치한다.
install.packages("arules")
library(arules)
trans <-  as.matrix( x, "Transaction")
trans
install.packages(c("rgl", "sna"))
library(sna)
library(rgl)
# 3. apriori 함수를 이용해서 연관관계를 분석한다.
rules1 <- apriori(trans, parameter=list(supp=0.2, conf=0.6, target="rules") )
rules1
trans
rules1
inspect(sort(rules1))
b2 <- t(as.matrix(trans)) %*% as.matrix(trans) # 희소행렬 생성
b2.w <- b2 - diag(diag(b2))
gplot(b2.w , displaylabel=T ,
vertex.cex=sqrt(diag(b2)) ,
vertex.col = "green" ,
edge.col="blue" ,
boxed.labels=F ,
arrowhead.cex = .3 ,
label.pos = 3 ,
edge.lwd = b2.w*2)
b2.w
# 1. 데이터를 로드합니다.
build <- read.csv("building.csv" , header = T)
# 2. na 를 0 으로 변경합니다.
build[is.na(build)] <- 0
# 3. 필요한 변수만 선별합니다.
build <- build[-1]
build
library(arules)
# 5. 연관규칙 모델을 생성합니다.
trans <- as.matrix(build , "Transaction")
rules1 <- apriori(trans , parameter = list(supp=0.2 , conf = 0.6 , target = "rules"))
rules1
# 6. 연관규칙을 확인합니다.
inspect(sort(rules1))
# 6. 연관규칙을 확인합니다.
inspect(sort(rules1))
# 7. 시각화를 합니다.
rules2 <- subset(rules1 , subset = lhs %pin% '보습학원' & confidence > 0.7)
inspect(sort(rules2))
rules3 <- subset(rules1 , subset = rhs %pin% '편의점' & confidence > 0.7)
rules3
inspect(sort(rules3))
#visualization
b2 <- t(as.matrix(build)) %*% as.matrix(build)
b2.w <- b2 - diag(diag(b2))
gplot(b2.w ,
displaylabel=T ,
vertex.cex=sqrt(diag(b2)) ,
vertex.col = "green" ,
edge.col="blue" ,
boxed.labels=F ,
arrowhead.cex = .3 ,
label.pos = 3 ,
edge.lwd = b2.w*2)
#rownames(b2.w)
#colnames(b2.w)
dev.new()
gplot(b2.w ,
displaylabel=T ,
vertex.cex=sqrt(diag(b2)) ,
vertex.col = "green" ,
edge.col="blue" ,
boxed.labels=F ,
arrowhead.cex = .3 ,
label.pos = 3 ,
edge.lwd = b2.w*2)
rownames(b2.w)
colnames(b2.w)
# 1. 이번주에 친구를 만난 횟수를 사회행렬로 구성함
paper <- read.csv("paper1.csv" , header = T)
paper[is.na(paper)] <- 0
paper
rownames(paper) <- paper[,1]
paper <- paper[-1]
paper2 <- as.matrix(paper)
paper2
# 2. 이번주에 개별적으로 책을 읽은 시간 데이터를 로드한다.
book <- read.csv("book_hour.csv" , header = T)
paper2
book
library(sna)
x11()
gplot(paper2 , displaylabels = T, boxed.labels = F , vertex.cex = sqrt(book[,2]) ,
vertex.col = "blue" , vertex.sides = 20 , edge.lwd = paper2*2 , edge.col = "green" , label.pos = 3)
paper <- read.csv("meal_11.csv" , header = T,encoding='UTF-8')
paper[is.na(paper)] <- 0
paper
paper <- read.csv("meal_11.csv" , header = T)
paper[is.na(paper)] <- 0
paper
paper <- read.csv("meal_11_m.csv" , header = T)
paper[is.na(paper)] <- 0
paper
paper <- read.csv("11_meal.csv" , header = T)
paper[is.na(paper)] <- 0
paper
paper <- read.csv("11_meal.csv" , header = T,encoding ='utf-8')
paper[is.na(paper)] <- 0
paper
paper <- read.csv("11_meal_m.csv" , header = T)
paper[is.na(paper)] <- 0
paper
rownames(paper) <- paper[,1]
paper <- paper[-1]
paper2 <- as.matrix(paper)
paper2
gplot(paper2 , displaylabels = T, boxed.labels = F ,
vertex.col = "blue" , vertex.sides = 20 , edge.lwd = paper2*2 , edge.col = "green" , label.pos = 3)
x11()
gplot(paper2 , displaylabels = T, boxed.labels = F ,
vertex.col = "blue" , vertex.sides = 20 , edge.lwd = paper2*2 , edge.col = "green" , label.pos = 3)
gplot(paper2 , displaylabels = T, boxed.labels = F ,
vertex.col = "blue" , vertex.sides = 2 , edge.lwd = paper2*2 , edge.col = "green" , label.pos = 3)
install.packages("networkD3")
library(networkD3)
library(dplyr)
# data set 소설 레미제라블 인물 관계도
# 1. 소설 장발장 데이터를 가져온다.
data(MisLinks, MisNodes)
head(MisNodes)  # 책 읽는 시간같은 데이터
head(MisLinks) # 인물끼리 몇번 만났는지 데이터
D3_network_LM<-forceNetwork(Links = MisLinks, Nodes = MisNodes,
Source = 'source', Target = 'target',
NodeID = 'name', Group = 'group',opacityNoHover = TRUE,
zoom = TRUE, bounded = TRUE,
fontSize = 15,
linkDistance = 75,
opacity = 0.9)
D3_network_LM
###########연습해 볼 데이터##################
links = '[
{"source": "Microsoft", "target": "Amazon", "type": "licensing"},
{"source": "Microsoft", "target": "HTC", "type": "licensing"},
{"source": "Samsung", "target": "Apple", "type": "suit"},
{"source": "Motorola", "target": "Apple", "type": "suit"},
{"source": "Nokia", "target": "Apple", "type": "resolved"},
{"source": "HTC", "target": "Apple", "type": "suit"},
{"source": "Kodak", "target": "Apple", "type": "suit"},
{"source": "Microsoft", "target": "Barnes & Noble", "type": "suit"},
{"source": "Microsoft", "target": "Foxconn", "type": "suit"},
{"source": "Oracle", "target": "Google", "type": "suit"},
{"source": "Apple", "target": "HTC", "type": "suit"},
{"source": "Microsoft", "target": "Inventec", "type": "suit"},
{"source": "Samsung", "target": "Kodak", "type": "resolved"},
{"source": "LG", "target": "Kodak", "type": "resolved"},
{"source": "RIM", "target": "Kodak", "type": "suit"},
{"source": "Sony", "target": "LG", "type": "suit"},
{"source": "Kodak", "target": "LG", "type": "resolved"},
{"source": "Apple", "target": "Nokia", "type": "resolved"},
{"source": "Qualcomm", "target": "Nokia", "type": "resolved"},
{"source": "Apple", "target": "Motorola", "type": "suit"},
{"source": "Microsoft", "target": "Motorola", "type": "suit"},
{"source": "Motorola", "target": "Microsoft", "type": "suit"},
{"source": "Huawei", "target": "ZTE", "type": "suit"},
{"source": "Ericsson", "target": "ZTE", "type": "suit"},
{"source": "Kodak", "target": "Samsung", "type": "resolved"},
{"source": "Apple", "target": "Samsung", "type": "suit"},
{"source": "Kodak", "target": "RIM", "type": "suit"},
{"source": "Nokia", "target": "Qualcomm", "type": "suit"}
]'
link_df = jsonlite::fromJSON(links)
# node의 index 숫자는 0부터 시작해야 한다
# dplyr::row_number()가 1부터 숫자를 매기기 때문에 거기서 1씩을 빼도록 한다
node_df = data.frame(node = unique(c(link_df$source, link_df$target))) %>% mutate(idx = row_number()-1)
# node_df에서 index값을 가져와서 source와 target에 해당하는 index 값을 저장한다
link_df = link_df %>%
left_join(node_df %>% rename(source_idx = idx), by=c('source' = 'node')) %>%
left_join(node_df %>% rename(target_idx = idx), by=c('target' = 'node'))
# 데이터 확인
node_df
link_df
D3_network_LM<-forceNetwork(Links = link_df,
Nodes = node_df,
Source = 'source_idx',
Target = 'target_idx',
NodeID = 'node',
Group = 'idx',
opacityNoHover = TRUE,
zoom = TRUE,
bounded = TRUE,
fontSize = 15,linkDistance = 75,
opacity = 0.9)
D3_network_LM
# 1.  라라랜드 데이터를 로드한다.
library(KoNLP)
library(wordcloud)
lala <- read.csv('라라랜드.csv', header=T, stringsAsFactors = F)
# 2. 영화평점이 9점이상은 긍정변수넣고 2점 이하는 부정변수에 넣는다
lala_positive <- lala[lala$score>=9,c('content')]
lala_negative <- lala[lala$score<=2,c('content')]
head(lala_positive)
head(lala_negative)
po <- sapply(lala_positive, extractNoun, USE.NAMES=F)
po2 <- unlist(po)
po2 <- Filter(function(x){nchar(x)>=2},po2)
po3 <- gsub('\\d+','',po2)
po3 <- gsub('관람객','',po3)
po3 <- gsub('평점', '', po3)
po3 <- gsub('영화', '', po3)
po3 <- gsub('진짜', '', po3)
po3 <- gsub('완전', '', po3)
po3 <- gsub('시간', '', po3)
po3 <- gsub('올해', '', po3)
po3 <- gsub('장면', '', po3)
po3 <- gsub('남자', '', po3)
po3 <- gsub('여자', '', po3)
po3 <- gsub('만큼', '', po3)
po3 <- gsub('니가', '', po3)
po3 <- gsub('년대', '', po3)
po3 <- gsub('옆사람', '', po3)
po3 <- gsub('들이', '', po3)
po3 <- gsub('저녁', '', po3)
write(unlist(po3), 'lala_positive.txt')
po4 <- read.table('lala_positive.txt')
po_wordcount <- table(po4)
ne <- sapply(lala_negative, extractNoun, USE.NAMES=F)
ne2 <- unlist(ne)
ne2 <- Filter(function(x){nchar(x)>=2},ne2)
ne3 <- gsub('\\d+','',ne2)
ne3 <- gsub('관람객','',ne3)
ne3 <- gsub('평점', '', ne3)
ne3 <- gsub('영화', '', ne3)
ne3 <- gsub('진짜', '', ne3)
ne3 <- gsub('완전', '', ne3)
ne3 <- gsub('시간', '', ne3)
ne3 <- gsub('올해', '', ne3)
ne3 <- gsub('장면', '', ne3)
ne3 <- gsub('남자', '', ne3)
ne3 <- gsub('여자', '', ne3)
ne3 <- gsub('만큼', '', ne3)
ne3 <- gsub('니가', '', ne3)
ne3 <- gsub('년대', '', ne3)
ne3 <- gsub('옆사람', '', ne3)
ne3 <- gsub('들이', '', ne3)
ne3 <- gsub('저녁', '', ne3)
write(unlist(ne3), 'lala_negative.txt')
ne4 <- read.table('lala_negative.txt')
ne_wordcount <- table(ne4)
# 5. 긍정 단어와 부정단어를 각각 워드 클라우드로 그려서 한 화면에 출력한다.
graphics.off()
palete <- brewer.pal(9,'Set1')
par(new=T, mfrow=c(1,2))
wordcloud(names(po_wordcount), freq=po_wordcount, scale=c(3,1), rot.per=0.1, random.order = F,
random.color = T, col=rainbow(15))
title(main='라라랜드의 긍정적인 평가', col.main='blue')
wordcloud(names(ne_wordcount), freq=ne_wordcount, scale=c(3,1), rot.per=0.1, random.order = F,
random.color = T, col=rainbow(15))
title(main='라라랜드의 부정적인 평가', col.main='red')
library(tm)
library(stringr)
library(arules)
lala <- read.csv('라라랜드.csv', header=T, stringsAsFactors = F)
lala_positive <- lala[lala$score>=9,c('content')]
lala_positive <- sapply(lala_positive, extractNoun, USE.NAMES=F)
head(lala_positive)
c <- unlist(lala_positive)
lala_positive2 <- Filter(function(x) { nchar(x) >= 2 & nchar(x) <= 5 }  , c)
#4. 데이터 정제작업( 분석하기에 너무 많이 나오는 단어를 삭제하는 작업 )
# 숫자제거
lala_positive2 <- gsub('\\d+','',lala_positive2)
lala_positive2 <- gsub('관람객','',lala_positive2)
lala_positive2 <- gsub('평점', '', lala_positive2)
lala_positive2 <- gsub('영화', '', lala_positive2)
lala_positive2 <- gsub('진짜', '', lala_positive2)
lala_positive2 <- gsub('완전', '', lala_positive2)
lala_positive2 <- gsub('시간', '', lala_positive2)
lala_positive2 <- gsub('올해', '', lala_positive2)
lala_positive2 <- gsub('장면', '', lala_positive2)
lala_positive2 <- gsub('남자', '', lala_positive2)
lala_positive2 <- gsub('여자', '', lala_positive2)
lala_positive2 <- gsub('만큼', '', lala_positive2)
lala_positive2 <- gsub('니가', '', lala_positive2)
lala_positive2 <- gsub('년대', '', lala_positive2)
lala_positive2 <- gsub('옆사람', '', lala_positive2)
lala_positive2 <- gsub('들이', '', lala_positive2)
lala_positive2 <- gsub('저녁', '', lala_positive2)
lala_positive2 <- gsub('영화', '', lala_positive2)
lala_positive2
#4. 데이터 정제작업( 분석하기에 너무 많이 나오는 단어를 삭제하는 작업 )
# 숫자제거
txt=readLines("lalagsub.txt")
cnt=length(txt)
i=1
for(i in 1:cnt){
lala_positive2=gsub((txt[i]),"",lala_positive2)
}
lala_positive2
res <- str_replace_all(lala_positive2, "[^[:alpha:]]","")
res <- res[res != ""]
wordcount <- table(res)
wordcount2 <- sort( table(res), decreasing=T)
keyword <- names( wordcount2[wordcount2>100] )
length(lala_positive)
length(lala_positive2)
#9. 아프리오리 분석을 위해서 표형태로 만드는 작업
contents <- c()
for(i in 1:length(lala_positive2)) {
inter <- intersect(lala_positive2[[i]] , keyword)
contents <- rbind(contents ,table(inter)[keyword])
}
colnames(contents) <- keyword
contents[which(is.na(contents))] <- 0
dim(lala_positive)
detach(package:tm, unload=T)
library(arules)
rules_lala <- apriori(contents , parameter = list(supp = 0.007 , conf = 0.3 , target = "rules"))
rules_lala
inspect(sort(rules_lala))
b2 <- t(as.matrix(contents)) %*% as.matrix(contents)
b2
b2.w <- b2 - diag(diag(b2))
b2.w
#rownames(b2.w)
#colnames(b2.w)
dev.new()
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "pink" ,
edge.col="light green" , boxed.labels=F ,
arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
par(new=T, mfrow=c(1,2))
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "pink" ,
edge.col="light green" , boxed.labels=F ,
arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "pink" ,
edge.col="light green" , boxed.labels=F ,
arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "pink" ,
edge.col="light green" , boxed.labels=F ,
arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*1.5)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "pink" ,
edge.col="light green" , boxed.labels=F ,
arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*1.5)
lala_positive2
lala <- read.csv('라라랜드.csv', header=T, stringsAsFactors = F)
lala_positive <- lala[lala$score>=9,c('content')]
lala_positive <- sapply(lala_positive, extractNoun, USE.NAMES=F)
c <- unlist(lala_positive)
lala_positive2 <- Filter(function(x){ nchar(x) >= 2 & nchar(x) <= 5 } , c)
lala_positive2 <- gsub('\\d+','',lala_positive2)
txt <- readLines("lalagsub.txt")
cnt <- length(txt)
txt
txt <- readLines("lalagsub.txt")
cnt <- length(txt)
txt
i=1
for(i in 1:cnt){
lala_positive2 <- gsub((txt[i]),'',lala_positive2)
}
lala_positive2
txt <- readLines("lalagsub.txt")
cnt <- length(txt)
txt
i=1
for(i in 1:cnt){
lala_positive2 <- gsub((txt[i]),'',lala_positive2)
}
lala_positive2
lala_positive2 <- Filter(function(x){ nchar(x) >= 2 & nchar(x) <= 5 } , c)
lala_positive2
lala_positive2 <- gsub('\\d+','',lala_positive2)
lala_positive2
res <- str_replace_all(lala_positive2, "[^[:alpha:]]","")
res <- res[res != ""]
res
wordcount <- table(res)
wordcount2 <- sort( table(res), decreasing=T)
keyword <- names( wordcount2[wordcount2>100] )
keyword
length(res)
contents <- c()
for(i in 1:length(res)) {
inter <- intersect(res[[i]] , keyword)
contents <- rbind(contents ,table(inter)[keyword])
}
colnames(contents) <- keyword
contents[which(is.na(contents))] <- 0
dim(lala_positive)
detach(package:tm, unload=T)
library(arules)
rules_lala <- apriori(contents , parameter = list(supp = 0.007 , conf = 0.3 , target = "rules"))
rules_lala
inspect(sort(rules_lala))
b2 <- t(as.matrix(contents)) %*% as.matrix(contents)
b2
b2.w <- b2 - diag(diag(b2))
b2.w
#rownames(b2.w)
#colnames(b2.w)
dev.new()
par(new=T, mfrow=c(1,2))
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*1.5)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "pink" ,
edge.col="light green" , boxed.labels=F ,
arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*1.5)
