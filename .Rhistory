library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
prop.table( table(ifelse(data1[(mm+1):nrow(data1_n),y]==result1,"o","x" )))
}
knn_fun()
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*2/3)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
prop.table( table(ifelse(data1[(mm+1):nrow(data1_n),y]==result1,"o","x" )))
}
knn_fun()
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*2/3)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
x <- data.frame(실제=wbcd_test_labels, 예측=result1)
}
knn_fun()
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*2/3)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
x <- data.frame(실제=wbcd_test_label, 예측=result1)
}
knn_fun()
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*2/3)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
x <- data.frame(실제=wbcd_test_label, 예측=result1)
}
knn_fun()
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*2/3)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
x <- data.frame(실제=data1_test_label, 예측=result1)
}
knn_fun()
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*2/3)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
x <- data.frame(실제=data1_test_label, 예측=result1)
x
}
knn_fun()
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*2/3)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
x <- data.frame(실제=data1_test_label, 예측=result1)
table(x)
}
knn_fun()
nrow(wine)
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*2/3)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
x <- data.frame(실제=data1_test_label, 예측=result1)
table(x)
temp<-c()
for ( i in 1:178 ) {
if  ( i%%2 != 0  ) {
data1_test_pred <- knn(train=data1_train, test=data1_test,
cl = data1_train_label,  k=i )
g2 <- CrossTable(x=data1_test_labels, y=data1_test_pred, chisq=FALSE)
g3 <- g2$prop.tbl[1] + g2$prop.tbl[4]
temp<-append(temp, g3 )
}
}
plot(temp, type='l', col='red')
}
knn_fun()
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*2/3)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
x <- data.frame(실제=data1_test_label, 예측=result1)
table(x)
temp<-c()
for ( i in 1:178 ) {
if  ( i%%2 != 0  ) {
data1_test_pred <- knn(train=data1_train, test=data1_test,
cl = data1_train_label,  k=i )
g2 <- CrossTable(x=data1_test_label, y=data1_test_pred, chisq=FALSE)
g3 <- g2$prop.tbl[1] + g2$prop.tbl[4]
temp<-append(temp, g3 )
}
}
plot(temp, type='l', col='red')
}
knn_fun()
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
data1$Type <- factor(data1$Type,
levels =c("t1","t2","t3"),
labels = c("t1","t2","t3"))
data1 <- data1[sample(nrow(data1)), ]
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*2/3)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
x <- data.frame(실제=data1_test_label, 예측=result1)
table(x)
temp<-c()
for ( i in 1:178 ) {
if  ( i%%2 != 0  ) {
data1_test_pred <- knn(train=data1_train, test=data1_test,
cl = data1_train_label,  k=i )
g2 <- CrossTable(x=data1_test_label, y=data1_test_pred, chisq=FALSE)
g3 <- g2$prop.tbl[1] + g2$prop.tbl[4]
temp<-append(temp, g3 )
}
}
plot(temp, type='l', col='red')
}
knn_fun()
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
data1$Type <- factor(data1$Type,
levels =c("t1","t2","t3"),
labels = c("t1","t2","t3"))
data1 <- data1[sample(nrow(data1)), ]
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*2/3)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
x <- data.frame(실제=data1_test_label, 예측=result1)
temp<-c()
for ( i in 1:178 ) {
if  ( i%%2 != 0  ) {
data1_test_pred <- knn(train=data1_train, test=data1_test,
cl = data1_train_label,  k=i )
g2 <- CrossTable(x=data1_test_label, y=data1_test_pred, chisq=FALSE)
g3 <- g2$prop.tbl[1] + g2$prop.tbl[4]
temp<-append(temp, g3 )
}
}
plot(temp, type='l', col='red')
table(x)
}
knn_fun()
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
data1$Type <- factor(data1$Type,
levels =c("t1","t2","t3"),
labels = c("t1","t2","t3"))
data1 <- data1[sample(nrow(data1)), ]
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*2/3)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
x <- data.frame(실제=data1_test_label, 예측=result1)
table(x)
}
knn_fun()
is.na(wine)
wine$Type
knn_fun()
knn_fun<-function(){
data1 <- read.csv("wine.csv", stringsAsFactors=FALSE)
y <-'Type'
k_n<-21
data1$Type <- factor(data1$Type,
levels =c("t1","t2","t3"),
labels = c("t1","t2","t3"))
data1 <- data1[sample(nrow(data1)), ]
normalize<-function(x) {
return( (x-min(x))/ ( max(x)-min(x)))
}
#data1 <- data1[-1]
#ncol1 <- which(colnames(data1)==y)
data1_n <- as.data.frame(lapply(data1[,-1], normalize) )
mm<-round(nrow(data1_n)*9/10)
data1_train <- data1_n[1:mm, ]
data1_test  <- data1_n[(mm+1):nrow(data1_n), ]
data1_train_label <- data1[1:mm,y]
data1_test_label  <- data1[(mm+1):nrow(data1_n),y]
library(class)
result1 <- knn(train=data1_train, test=data1_test,  cl= data1_train_label, k = k_n )
x <- data.frame(실제=data1_test_label, 예측=result1)
table(x)
}
knn_fun()
runApp('auto.R')
runApp('auto.R')
runApp('C:/Users/knitwill/Desktop/auto2.R')
# 1. 버섯 데이터를 R 로 로드한다.
mushroom <- read.csv("mushrooms.csv", header=T, stringsAsFactors=T)
View(mushroom)
# 2. 8124 독버섯 데이터만 따로 빼서 mush_test.csv 로 저장한다.
mush_test <- mushroom[8123, ]
mush_test
write.csv( mush_test, "mush_test.csv",row.names=FALSE )
nrow(mushroom)
mush_test <- mushroom[8123, ]
mush_test
write.csv( mush_test, "mush_test.csv",row.names=FALSE )
nrow(mushroom)
mushrooms <- mushroom[ -8123,  ]
nrow(mushrooms)
# 4. mushrooms 데이터를 훈련 데이터와 테스트 데이터로 나눈다 ( 훈련 데이터는 75%,  테스트 데이터는 25% )
set.seed(1)
dim(mushrooms) # 차원 확인인
train_cnt <- round( 0.75*dim(mushrooms)[1] )
train_cnt
train_index <- sample( 1:dim(mushrooms)[1], train_cnt, replace=F) # 6092개 생성하여 인덱스 생성
train_index
mushrooms_train <- mushrooms[ train_index,  ]
mushrooms_test <- mushrooms[- train_index,  ]
nrow(mushrooms_train)  #  6092
nrow(mushrooms_test)    #  2031
str(mushrooms_train)
# 5. 나이브 베이즈 알고리즘으로 독버섯과 일반 버섯을 분류하는 모델을 생성한다.
library(e1071) #        모든 컬럼들
#                          ↓
model1 <- naiveBayes(type~ . ,  data=mushrooms_train)
#                     ↑
#                   라벨 컬럼명
model1
# 6. 위에서 만든 모델과 테스트 데이터를 가지고 독버섯과 일반버섯을 잘 분류하는지 예측해 본다.
result1 <- predict( model1, mushrooms_test[  , -1] ) # label 제외
result1
# 7. 이원 교차표를 그려서 최종 분류 결과를 확인한다.
library(gmodels)
CrossTable( mushrooms_test[  ,1], result1)
# 8. 위의 모델의 성능을 올리시오 !
model2 <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=0.0004)
result2 <- predict( model2, mushrooms_test[ , -1] )
CrossTable( mushrooms_test[ ,1], result2)
result3 <- predict( model2, mush_test )
result3
model_test_pred <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=0.0001)
g2 <- CrossTable(x=mushrooms_test, y=model_test_pred, chisq=FALSE)
model_test_pred <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=0.0001)
rs <- predict( model_test_pred, mushrooms_test[,-1])
g2 <- CrossTable(x=mushrooms_test, y=rs, chisq=FALSE)
mushrooms_test
View(mushrooms_test)
nrow(mushrooms_test)
nrow(model_test_pred)
model_test_pred <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=0.0001)
nrow(model_test_pred)
rs <- predict( model_test_pred, mushrooms_test[,-1])
nrow(rs)
# 1. 버섯 데이터를 R 로 로드한다.
mushroom <- read.csv("mushrooms.csv", header=T, stringsAsFactors=T)
View(mushroom)
########################## 2단계
# 명목형 변수이기 때문에 이상치 X
########################## 3단계
# 2. 8124 독버섯 데이터만 따로 빼서 mush_test.csv 로 저장한다.
mush_test <- mushroom[8123, ]
mush_test
write.csv( mush_test, "mush_test.csv",row.names=FALSE )
# 3. 8124 독버섯 데이터를 훈련 데이터에서 제외 시키시오 !
nrow(mushroom)
mushrooms <- mushroom[ -8123,  ]
nrow(mushrooms)
# 4. mushrooms 데이터를 훈련 데이터와 테스트 데이터로 나눈다 ( 훈련 데이터는 75%,  테스트 데이터는 25% )
set.seed(1)
dim(mushrooms) # 차원 확인
train_cnt <- round( 0.75*dim(mushrooms)[1] )
train_cnt  # 6092
train_index <- sample( 1:dim(mushrooms)[1], train_cnt, replace=F) # 6092개 랜덤 생성하여 index 생성
train_index
mushrooms_train <- mushrooms[ train_index,  ]
mushrooms_test <- mushrooms[- train_index,  ] # train에 들어간 index번호를 제외한 나머지만 test로 부여
nrow(mushrooms_train)  #  6092
nrow(mushrooms_test)    #  2031
str(mushrooms_train)
# 5. 나이브 베이즈 알고리즘으로 독버섯과 일반 버섯을 분류하는 모델을 생성한다.
library(e1071) #        모든 컬럼들
#                          ↓
model1 <- naiveBayes(type~ . ,  data=mushrooms_train)
#                     ↑
#                   라벨 컬럼명
model1 # 우도표 생성
# 6. 위에서 만든 모델과 테스트 데이터를 가지고 독버섯과 일반버섯을 잘 분류하는지 예측해 본다.
result1 <- predict( model1, mushrooms_test[  , -1] ) # label 제외
result1
# 7. 이원 교차표를 그려서 최종 분류 결과를 확인한다.
library(gmodels)
CrossTable( mushrooms_test[  ,1], result1)
nrow(result1)
nrow(result1)
lenght(result1)
length(result1)
model_test_pred <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=0.0001)
rs <- predict( model_test_pred, mushrooms_test[,-1])
length(rs)
g2 <- CrossTable(x=mushrooms_test, y=rs, chisq=FALSE)
length(mushrooms_test)
length(mushrooms_test)
model_test_pred <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=0.0001)
rs <- predict( model_test_pred, mushrooms_test[,-1])
length(mushrooms_test[,-1])
nrow(mushrooms_test)    #  2031
#                          ↓
model1 <- naiveBayes(type~ . ,  data=mushrooms_train)
# 8. 위의 모델의 성능을 올리시오 !
model2 <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=0.0004)
model_test <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=0.0001)
rs <- predict( model_test, mushrooms_test[,-1])
g2 <- CrossTable(x=mushrooms_test, y=rs, chisq=FALSE)
rs
g2 <- CrossTable(mushrooms_test, rs)
length(mushrooms_test)
nrow(mushrooms_test)
nrow(rs)
length(rs)
rs
mushrooms_test
mushrooms_test[,-1]
model_test <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=0.0001)
model_test
testfn <- CrossTable(mushrooms_test, test_rs)
model_test <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=0.0001)
test_rs <- predict( model_test, mushrooms_test[,-1])
testfn <- CrossTable(mushrooms_test, test_rs)
result2
testfn <- CrossTable(mushrooms_test[,1], test_rs)
temp<-c()
for ( i in 0.0001:0.001 ) {
model_test_pred <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=i)
rs <- predict( model_test_pred, mushrooms_test[,-1])
g2 <- CrossTable(x=mushrooms_test[,1], y=model_test_pred, chisq=FALSE)
g3 <- g2$prop.tbl[1] + g2$prop.tbl[4]
temp<-append(temp, g3 )
}
temp
plot(temp, type='l', col='red')
temp<-c()
for ( i in 1:1000 ) {
model_test_pred <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=i/1000)
rs <- predict( model_test_pred, mushrooms_test[,-1])
g2 <- CrossTable(x=mushrooms_test[,1], y=model_test_pred, chisq=FALSE)
g3 <- g2$prop.tbl[1] + g2$prop.tbl[4]
temp<-append(temp, g3 )
}
temp
temp<-c()
for ( i in 1:100 ) {
model_test_pred <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=i/10000)
rs <- predict( model_test_pred, mushrooms_test[,-1])
g2 <- CrossTable(x=mushrooms_test[,1], y=model_test_pred, chisq=FALSE)
g3 <- g2$prop.tbl[1] + g2$prop.tbl[4]
temp<-append(temp, g3 )
}
temp<-c()
for ( i in 1:100 ) {
model_test_pred <- naiveBayes(type~ . ,  data=mushrooms_train, laplace=i/10000)
rs <- predict( model_test_pred, mushrooms_test[,-1])
g2 <- CrossTable(x=mushrooms_test[,1], y=rs, chisq=FALSE)
g3 <- g2$prop.tbl[1] + g2$prop.tbl[4]
temp<-append(temp, g3 )
}
temp
plot(temp, type='l', col='red')
