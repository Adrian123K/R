<<<<<<< HEAD
naive_fun<-function(){
library(e1071)
flu <- read.csv("flu.csv", header=T, stringsAsFactors=TRUE)
train <- flu[1:nrow(flu),-1]
model <- naiveBayes(train[,1:4], train$flue , laplace=0)
fname <- file.choose()
test <- read.csv(fname, header=T, stringsAsFactor=F )
result <- predict( model, test, type='raw')
print(paste('독감 환자일 확률이',round(result[2]*100,digits=1),'% 입니다.'))
}
x1 <- menu( c('산포도 그래프','히스토그램 그래프','사분위수 그래프', '유방암 진단', '독감 진단') ,
title='숫자를 선택하세요: ' )
switch ( x1,
san1 = {   my_scatter()       },
san2 = {   my_hist()          },
san3 = {   my_box()           },
san4 = {   knn_fun()          },
san5 = {   naive_fun()        }
)
}
my_func()
(-11/16)*log2(11/16)+(-5/16)*log2(5/16)
(-10/16)*log2(10/16)+(-6/16)*log2(6/16)
((-2/3)*log2(2/3)+(-1/3)*log2(1/3))*3/5
skin <- read.csv('skin.csv',head=T)
skin
str(skin)
skin <- read.csv('skin.csv',head=T, stringsAsFactors = T)
str(skin)
head(skin)
install.packages('FSelector')
library(FSelector)
weights <- information.gain(cupon_react~., skin, unit='log2')
print(weights)
weights <- information.gain(cupon_react~gender, skin, unit='log2')
print(weights)
weights <- information.gain(cupon_react~skin[,-1], skin, unit='log2')
weights <- information.gain(cupon_react~., skin, unit='log2')
print(weights)
skin <- read.csv('skin.csv',head=T, stringsAsFactors = T)
head(skin)
weights <- information.gain(cupon_react~., skin, unit='log2')
print(weights)
skin <- read.csv('skin.csv',header=T, stringsAsFactors = T)
weights <- information.gain(cupon_react~., skin, unit='log2')
print(weights)
print(weights)
fat <- read.csv('fatliver2.csv',head=T,stringsAsFactors = T)
fat
str(fat)
rs2 <- information.gain(fatliver~., fat, unit='log2')
rs2 <- information.gain(FATLIVER~., fat, unit='log2')
print(rs)
rs <- information.gain(FATLIVER~., fat, unit='log2')
print(rs)
# 1. 의사결정 패키지인 C50 패키지를 설치한다.
install.packages("C50")
library(C50)
# 2. 백화점 화장품 고객 데이터를 로드하고 shuffle 한다.
skin <- read.csv("skin.csv", header=T )
nrow(skin)
skin_real_test_cust <- skin[30,  ] # 모델 생성 후 정확도를 올린 후 최종적으로 모델이 잘 맞추는지 확인하기 위해 한 건 제외
skin2 <-  skin[ 1:29, ]
nrow(skin2)
skin_real_test_cust
skin2 <- skin2[ , -1] # 고객번호를 제외시킨다.
set.seed(11)
skin2_shuffle <- skin2[sample(nrow(skin2)),    ]  # shuffle 시킴
# 3. 화장품 고객 데이터를 7대 3로 train 과 test 로 나눈다.
train_num <-  round(0.7 * nrow(skin2_shuffle), 0)
skin2_train <- skin2_shuffle[1:train_num,  ]
skin2_test  <- skin2_shuffle[(train_num+1) : nrow(skin2_shuffle), ]
nrow(skin2_train)  # 20
nrow(skin2_test)   #  9
# 4. C50 패키지를 이용해서 분류 모델을 생성한다.
skin_model <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react )
# 4. C50 패키지를 이용해서 분류 모델을 생성한다.
skin_model <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react )
# 2. 백화점 화장품 고객 데이터를 로드하고 shuffle 한다.
skin <- read.csv("skin.csv", header=T,stringsAsFactors = T )
nrow(skin)
skin_real_test_cust <- skin[30,  ] # 모델 생성 후 정확도를 올린 후 최종적으로 모델이 잘 맞추는지 확인하기 위해 한 건 제외
skin2 <-  skin[ 1:29, ]
nrow(skin2)
skin_real_test_cust
skin2 <- skin2[ , -1] # 고객번호를 제외시킨다.
set.seed(11)
skin2_shuffle <- skin2[sample(nrow(skin2)),    ]  # shuffle 시킴
# 3. 화장품 고객 데이터를 7대 3로 train 과 test 로 나눈다.
train_num <-  round(0.7 * nrow(skin2_shuffle), 0)
skin2_train <- skin2_shuffle[1:train_num,  ]
skin2_test  <- skin2_shuffle[(train_num+1) : nrow(skin2_shuffle), ]
nrow(skin2_train)  # 20
nrow(skin2_test)   #  9
# 4. C50 패키지를 이용해서 분류 모델을 생성한다.
skin_model <- C5.0(skin2_train[,-6],  skin2_train$cupon_react )
# 5. 위에서 만든 skin_model 를 이용해서 테스테 데이터의 라벨을 예측하시오!
skin2_result  <- predict( skin_model , skin2_test[  , -6])
# 5. 위에서 만든 skin_model 를 이용해서 테스테 데이터의 라벨을 예측하시오!
skin2_result  <- predict( skin_model , skin2_test[  , -6])
skin2_test
# 4. C50 패키지를 이용해서 분류 모델을 생성한다.
skin_model <- C5.0(skin2_train[,-6],  skin2_train$cupon_react )
# 5. 위에서 만든 skin_model 를 이용해서 테스테 데이터의 라벨을 예측하시오!
skin2_result  <- predict( skin_model , skin2_test[  , -6])
# 4. C50 패키지를 이용해서 분류 모델을 생성한다.
skin_model <- C5.0(skin2_train[,-6],  skin2_train$cupon_react,type='class' )
# 5. 위에서 만든 skin_model 를 이용해서 테스테 데이터의 라벨을 예측하시오!
skin2_result  <- predict( skin_model , skin2_test[  , -6])
# 4. C50 패키지를 이용해서 분류 모델을 생성한다.
skin_model <- C5.0(skin2_train[,-6],  skin2_train$cupon_react )
# 5. 위에서 만든 skin_model 를 이용해서 테스테 데이터의 라벨을 예측하시오!
skin2_result  <- predict( skin_model , skin2_test[  , -6],type='class')
# 5. 위에서 만든 skin_model 를 이용해서 테스테 데이터의 라벨을 예측하시오!
skin2_result  <- predict( skin_model , skin2_test[,-6])
skin <- read.csv("skin.csv", header=T ,stringsAsFactors = TRUE)
str(skin)
nrow(skin)
skin_real_test_cust <- skin[30,  ]  # 나중에 모델 만들고 정확도를 올린후에
# 최종적으로 모델이 잘 맞추는지 확인하려
# 한건 제외 시킨다.
skin2 <-  skin[ 1:29, ]
nrow(skin2)
skin_real_test_cust
skin2 <- skin2[ , -1] # 고객번호를 제외시킨다.
set.seed(11)
skin2_shuffle <- skin2[sample(nrow(skin2)),    ]  # shuffle 시킴
train_num <-  round(0.7 * nrow(skin2_shuffle), 0)
skin2_train <- skin2_shuffle[1:train_num,  ]
skin2_test  <- skin2_shuffle[(train_num+1) : nrow(skin2_shuffle), ]
nrow(skin2_train)  # 20
nrow(skin2_test)   #  9
library(C50)
skin_model <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react )
skin2_result  <- predict( skin_model , skin2_test[  , -6])
skin <- read.csv("skin.csv", head=T ,stringsAsFactors = T)
skin_real_test_cust <- skin[30,  ]  # 나중에 모델 만들고 정확도를 올린후에
# 최종적으로 모델이 잘 맞추는지 확인하려
# 한건 제외 시킨다.
skin2 <-  skin[ 1:29, ]
skin_real_test_cust
skin2 <- skin2[ , -1] # 고객번호를 제외시킨다.
set.seed(11)
skin2_shuffle <- skin2[sample(nrow(skin2)),    ]  # shuffle 시킴
train_num <-  round(0.7 * nrow(skin2_shuffle), 0)
skin2_train <- skin2_shuffle[1:train_num,  ]
skin2_test  <- skin2_shuffle[(train_num+1) : nrow(skin2_shuffle), ]
skin_model <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react )
skin2_result  <- predict( skin_model , skin2_test[  , -6])
library(gmodels)
CrossTable( skin2_test[  , 6],  skin2_result )
skin_model2 <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react,trials=10 )
skin2_result  <- predict( skin_model , skin2_test[  , -6])
CrossTable( skin2_test[  , 6],  skin2_result )
skin2_result  <- predict( skin_model2 , skin2_test[  , -6])
CrossTable( skin2_test[  , 6],  skin2_result )
skin_model2 <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react,trials=10 )
skin2_result  <- predict( skin_model2 , skin2_test[  , -6])
CrossTable( skin2_test[  , 6],  skin2_result )
skin <- read.csv("skin.csv", head=T ,stringsAsFactors = T)
skin_real_test_cust <- skin[30,  ]  # 나중에 모델 만들고 정확도를 올린후에 최종적으로 모델이 잘 맞추는지 확인하려 한건 제외 시킨다.
skin2 <-  skin[ 1:29, ]
skin_real_test_cust
skin2 <- skin2[ , -1] # 고객번호를 제외시킨다.
set.seed(11)
skin2_shuffle <- skin2[sample(nrow(skin2)),    ]  # shuffle 시킴
train_num <-  round(0.7 * nrow(skin2_shuffle), 0)
skin2_train <- skin2_shuffle[1:train_num,  ]
skin2_test  <- skin2_shuffle[(train_num+1) : nrow(skin2_shuffle), ]
library(C50)
skin_model <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react )
skin_model2 <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react,trials=10 )
skin2_result  <- predict( skin_model2 , skin2_test[  , -6])
CrossTable( skin2_test[  , 6],  skin2_result )
skin_model2 <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react,trials=14 )
skin2_result  <- predict( skin_model2 , skin2_test[  , -6])
CrossTable( skin2_test[  , 6],  skin2_result )
skin <- read.csv("skin.csv", head=T ,stringsAsFactors = T)
skin_real_test_cust <- skin[30,  ]  # 나중에 모델 만들고 정확도를 올린후에 최종적으로 모델이 잘 맞추는지 확인하려 한건 제외 시킨다.
skin2 <-  skin[ 1:29, ]
skin_real_test_cust
skin2 <- skin2[ , -1] # 고객번호를 제외시킨다.
set.seed(20)
skin2_shuffle <- skin2[sample(nrow(skin2)),    ]  # shuffle 시킴
train_num <-  round(0.7 * nrow(skin2_shuffle), 0)
skin2_train <- skin2_shuffle[1:train_num,  ]
skin2_test  <- skin2_shuffle[(train_num+1) : nrow(skin2_shuffle), ]
skin_model2 <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react,trials=10 )
skin2_result  <- predict( skin_model2 , skin2_test[  , -6])
CrossTable( skin2_test[  , 6],  skin2_result )
credit <- read.csv("credit.csv",head=T)
str(credit)
credit <- read.csv("credit.csv",head=T,stringsAsFactors = T)
=======
return((x - min(x)) / (max(x) - min(x)))
}
boston_norm <- as.data.frame(lapply(boston, normalize))
summary(boston_norm$MEDV)
summary(boston$MEDV)
hist(boston$MEDV)
dim(boston_norm)
set.seed(1)
s_cnt<-round(0.7*(nrow(boston_norm)))
s_index<-sample(1:nrow(boston_norm), s_cnt, replace=F)
boston_train <- boston_norm[s_index, ]
boston_test <- boston_norm[-s_index, ]
head(boston_train)
library(neuralnet)
boston_model <- neuralnet(MEDV~CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT, data=boston_train, hidden=10)
model_results <- compute(boston_model, boston_test[1:13])
predicted_houseprice <- model_results$net.result
cor(predicted_houseprice, boston_test$MEDV)
boston_model2 <- neuralnet(MEDV~CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT,
data=boston_train, hidden=c(10, 5), rep=10)
model_results2 <- compute(boston_model2, boston_test[1:13])
predicted_houseprice2 <- model_results2$net.result
cor(predicted_houseprice2, boston_test$MEDV)
sys.setlocale("LC_COLLATE", "ko_KR.UTF-8")
setwd("d:/R")
credit <- read.csv("credit.csv",  stringsAsFactors = TRUE)
>>>>>>> 0a0c2ac25db7f0907364d1d7736865daa27f0e0f
str(credit)
prop.table( table(credit$default)  )
hist( credit$amount, col="grey", density=80)
summary( credit$amount)
set.seed(0)
credit_shuffle <-  credit[ sample( nrow(credit) ),  ]
train_num <- round( 0.9 * nrow(credit_shuffle), 0)
credit_train <- credit_shuffle[1:train_num ,  ]
credit_test  <- credit_shuffle[(train_num+1) : nrow(credit_shuffle),  ]
library(C50)
credit_model <- C5.0( credit_train[ ,-17] , credit_train[  , 17] )
credit_result <-  predict( credit_model, credit_test[  , -17] )
library(gmodels)
CrossTable(  credit_test[   , 17], credit_result )
library(ipred)
set.seed(300)
mybag <- bagging(default ~ ., data = credit, nbagg = 25)
credit_pred <- predict(mybag, credit)
table(credit_pred, credit$default)
library(caret)
set.seed(300)
ctrl <- trainControl(method = "cv", number = 10)
train(default ~ ., data = credit, method = "treebag",  trControl = ctrl)
Sys.setlocale("LC_COLLATE", "ko_KR.UTF-8")
setwd("d:/R")
boston<-read.csv("boston.csv", stringsAsFactors=T)
str(boston)
head(boston)
summary(boston$MEDV)
hist(boston$MEDV)
boston<-read.csv("boston.csv", stringsAsFactors=T)
str(boston)
head(boston)
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
boston_norm <- as.data.frame(lapply(boston, normalize))
summary(boston_norm$MEDV)
summary(boston$MEDV)
hist(boston$MEDV)
dim(boston_norm)
set.seed(1)
s_cnt<-round(0.7*(nrow(boston_norm)))
s_index<-sample(1:nrow(boston_norm), s_cnt, replace=F)
boston_train <- boston_norm[s_index, ]
boston_test <- boston_norm[-s_index, ]
head(boston_train)
library(neuralnet)
boston_model <- neuralnet(MEDV~CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT, data=boston_train, hidden=10)
model_results <- compute(boston_model, boston_test[1:13])
predicted_houseprice <- model_results$net.result
cor(predicted_houseprice, boston_test$MEDV)
boston_model2 <- neuralnet(MEDV~CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT,
data=boston_train, hidden=c(10, 5), rep=10)
model_results2 <- compute(boston_model2, boston_test[1:13])
predicted_houseprice2 <- model_results2$net.result
cor(predicted_houseprice2, boston_test$MEDV)
library(devtools)
library(tidyverse)
install.packages('tidyverse')
library(tidyverse)
library(datarium)
install.packages('datarium')
library(datarium)
data("marketing", package = "datarium")
head(marketing, 4)
model <- lm(sales ~ youtube + facebook + newspaper, data = marketing) summary(model)
model <- lm(sales ~ youtube + facebook + newspaper, data = marketing)
summary(model)
x <- data.frame(beer=c(0,1,1,1,0),
bread=c(1,1,0,1,1),
cola=c(0,0,1,0,1),
diapers=c(0,1,1,1,1),
eggs=c(0,1,0,0,0),
milk=c(1,0,1,1,1) )
x
# 2. arules 패키지를 설치한다.
install.packages("arules")
library(arules)
trans <-  as.matrix( x, "Transaction")
trans
install.packages(c("rgl", "sna"))
library(sna)
library(rgl)
# 3. apriori 함수를 이용해서 연관관계를 분석한다.
rules1 <- apriori(trans, parameter=list(supp=0.2, conf=0.6, target="rules") )
rules1
trans
rules1
inspect(sort(rules1))
b2 <- t(as.matrix(trans)) %*% as.matrix(trans) # 희소행렬 생성
b2.w <- b2 - diag(diag(b2))
gplot(b2.w , displaylabel=T ,
vertex.cex=sqrt(diag(b2)) ,
vertex.col = "green" ,
edge.col="blue" ,
boxed.labels=F ,
arrowhead.cex = .3 ,
label.pos = 3 ,
edge.lwd = b2.w*2)
b2.w
# 1. 데이터를 로드합니다.
build <- read.csv("building.csv" , header = T)
# 2. na 를 0 으로 변경합니다.
build[is.na(build)] <- 0
# 3. 필요한 변수만 선별합니다.
build <- build[-1]
build
library(arules)
# 5. 연관규칙 모델을 생성합니다.
trans <- as.matrix(build , "Transaction")
rules1 <- apriori(trans , parameter = list(supp=0.2 , conf = 0.6 , target = "rules"))
rules1
# 6. 연관규칙을 확인합니다.
inspect(sort(rules1))
# 6. 연관규칙을 확인합니다.
inspect(sort(rules1))
# 7. 시각화를 합니다.
rules2 <- subset(rules1 , subset = lhs %pin% '보습학원' & confidence > 0.7)
inspect(sort(rules2))
rules3 <- subset(rules1 , subset = rhs %pin% '편의점' & confidence > 0.7)
rules3
inspect(sort(rules3))
#visualization
b2 <- t(as.matrix(build)) %*% as.matrix(build)
b2.w <- b2 - diag(diag(b2))
gplot(b2.w ,
displaylabel=T ,
vertex.cex=sqrt(diag(b2)) ,
vertex.col = "green" ,
edge.col="blue" ,
boxed.labels=F ,
arrowhead.cex = .3 ,
label.pos = 3 ,
edge.lwd = b2.w*2)
#rownames(b2.w)
#colnames(b2.w)
dev.new()
gplot(b2.w ,
displaylabel=T ,
vertex.cex=sqrt(diag(b2)) ,
vertex.col = "green" ,
edge.col="blue" ,
boxed.labels=F ,
arrowhead.cex = .3 ,
label.pos = 3 ,
edge.lwd = b2.w*2)
rownames(b2.w)
colnames(b2.w)
# 1. 이번주에 친구를 만난 횟수를 사회행렬로 구성함
paper <- read.csv("paper1.csv" , header = T)
paper[is.na(paper)] <- 0
paper
rownames(paper) <- paper[,1]
paper <- paper[-1]
paper2 <- as.matrix(paper)
paper2
# 2. 이번주에 개별적으로 책을 읽은 시간 데이터를 로드한다.
book <- read.csv("book_hour.csv" , header = T)
paper2
book
library(sna)
x11()
gplot(paper2 , displaylabels = T, boxed.labels = F , vertex.cex = sqrt(book[,2]) ,
vertex.col = "blue" , vertex.sides = 20 , edge.lwd = paper2*2 , edge.col = "green" , label.pos = 3)
paper <- read.csv("meal_11.csv" , header = T,encoding='UTF-8')
paper[is.na(paper)] <- 0
paper
paper <- read.csv("meal_11.csv" , header = T)
paper[is.na(paper)] <- 0
paper
paper <- read.csv("meal_11_m.csv" , header = T)
paper[is.na(paper)] <- 0
paper
paper <- read.csv("11_meal.csv" , header = T)
paper[is.na(paper)] <- 0
paper
paper <- read.csv("11_meal.csv" , header = T,encoding ='utf-8')
paper[is.na(paper)] <- 0
paper
paper <- read.csv("11_meal_m.csv" , header = T)
paper[is.na(paper)] <- 0
paper
rownames(paper) <- paper[,1]
paper <- paper[-1]
paper2 <- as.matrix(paper)
paper2
gplot(paper2 , displaylabels = T, boxed.labels = F ,
vertex.col = "blue" , vertex.sides = 20 , edge.lwd = paper2*2 , edge.col = "green" , label.pos = 3)
x11()
gplot(paper2 , displaylabels = T, boxed.labels = F ,
vertex.col = "blue" , vertex.sides = 20 , edge.lwd = paper2*2 , edge.col = "green" , label.pos = 3)
gplot(paper2 , displaylabels = T, boxed.labels = F ,
vertex.col = "blue" , vertex.sides = 2 , edge.lwd = paper2*2 , edge.col = "green" , label.pos = 3)
install.packages("networkD3")
library(networkD3)
library(dplyr)
# data set 소설 레미제라블 인물 관계도
# 1. 소설 장발장 데이터를 가져온다.
data(MisLinks, MisNodes)
head(MisNodes)  # 책 읽는 시간같은 데이터
head(MisLinks) # 인물끼리 몇번 만났는지 데이터
D3_network_LM<-forceNetwork(Links = MisLinks, Nodes = MisNodes,
Source = 'source', Target = 'target',
NodeID = 'name', Group = 'group',opacityNoHover = TRUE,
zoom = TRUE, bounded = TRUE,
fontSize = 15,
linkDistance = 75,
opacity = 0.9)
D3_network_LM
###########연습해 볼 데이터##################
links = '[
{"source": "Microsoft", "target": "Amazon", "type": "licensing"},
{"source": "Microsoft", "target": "HTC", "type": "licensing"},
{"source": "Samsung", "target": "Apple", "type": "suit"},
{"source": "Motorola", "target": "Apple", "type": "suit"},
{"source": "Nokia", "target": "Apple", "type": "resolved"},
{"source": "HTC", "target": "Apple", "type": "suit"},
{"source": "Kodak", "target": "Apple", "type": "suit"},
{"source": "Microsoft", "target": "Barnes & Noble", "type": "suit"},
{"source": "Microsoft", "target": "Foxconn", "type": "suit"},
{"source": "Oracle", "target": "Google", "type": "suit"},
{"source": "Apple", "target": "HTC", "type": "suit"},
{"source": "Microsoft", "target": "Inventec", "type": "suit"},
{"source": "Samsung", "target": "Kodak", "type": "resolved"},
{"source": "LG", "target": "Kodak", "type": "resolved"},
{"source": "RIM", "target": "Kodak", "type": "suit"},
{"source": "Sony", "target": "LG", "type": "suit"},
{"source": "Kodak", "target": "LG", "type": "resolved"},
{"source": "Apple", "target": "Nokia", "type": "resolved"},
{"source": "Qualcomm", "target": "Nokia", "type": "resolved"},
{"source": "Apple", "target": "Motorola", "type": "suit"},
{"source": "Microsoft", "target": "Motorola", "type": "suit"},
{"source": "Motorola", "target": "Microsoft", "type": "suit"},
{"source": "Huawei", "target": "ZTE", "type": "suit"},
{"source": "Ericsson", "target": "ZTE", "type": "suit"},
{"source": "Kodak", "target": "Samsung", "type": "resolved"},
{"source": "Apple", "target": "Samsung", "type": "suit"},
{"source": "Kodak", "target": "RIM", "type": "suit"},
{"source": "Nokia", "target": "Qualcomm", "type": "suit"}
]'
link_df = jsonlite::fromJSON(links)
# node의 index 숫자는 0부터 시작해야 한다
# dplyr::row_number()가 1부터 숫자를 매기기 때문에 거기서 1씩을 빼도록 한다
node_df = data.frame(node = unique(c(link_df$source, link_df$target))) %>% mutate(idx = row_number()-1)
# node_df에서 index값을 가져와서 source와 target에 해당하는 index 값을 저장한다
link_df = link_df %>%
left_join(node_df %>% rename(source_idx = idx), by=c('source' = 'node')) %>%
left_join(node_df %>% rename(target_idx = idx), by=c('target' = 'node'))
# 데이터 확인
node_df
link_df
D3_network_LM<-forceNetwork(Links = link_df,
Nodes = node_df,
Source = 'source_idx',
Target = 'target_idx',
NodeID = 'node',
Group = 'idx',
opacityNoHover = TRUE,
zoom = TRUE,
bounded = TRUE,
fontSize = 15,linkDistance = 75,
opacity = 0.9)
D3_network_LM
# 1.  라라랜드 데이터를 로드한다.
library(KoNLP)
library(wordcloud)
lala <- read.csv('라라랜드.csv', header=T, stringsAsFactors = F)
# 2. 영화평점이 9점이상은 긍정변수넣고 2점 이하는 부정변수에 넣는다
lala_positive <- lala[lala$score>=9,c('content')]
lala_negative <- lala[lala$score<=2,c('content')]
head(lala_positive)
head(lala_negative)
po <- sapply(lala_positive, extractNoun, USE.NAMES=F)
po2 <- unlist(po)
po2 <- Filter(function(x){nchar(x)>=2},po2)
po3 <- gsub('\\d+','',po2)
po3 <- gsub('관람객','',po3)
po3 <- gsub('평점', '', po3)
po3 <- gsub('영화', '', po3)
po3 <- gsub('진짜', '', po3)
po3 <- gsub('완전', '', po3)
po3 <- gsub('시간', '', po3)
po3 <- gsub('올해', '', po3)
po3 <- gsub('장면', '', po3)
po3 <- gsub('남자', '', po3)
po3 <- gsub('여자', '', po3)
po3 <- gsub('만큼', '', po3)
po3 <- gsub('니가', '', po3)
po3 <- gsub('년대', '', po3)
po3 <- gsub('옆사람', '', po3)
po3 <- gsub('들이', '', po3)
po3 <- gsub('저녁', '', po3)
write(unlist(po3), 'lala_positive.txt')
po4 <- read.table('lala_positive.txt')
po_wordcount <- table(po4)
ne <- sapply(lala_negative, extractNoun, USE.NAMES=F)
ne2 <- unlist(ne)
ne2 <- Filter(function(x){nchar(x)>=2},ne2)
ne3 <- gsub('\\d+','',ne2)
ne3 <- gsub('관람객','',ne3)
ne3 <- gsub('평점', '', ne3)
ne3 <- gsub('영화', '', ne3)
ne3 <- gsub('진짜', '', ne3)
ne3 <- gsub('완전', '', ne3)
ne3 <- gsub('시간', '', ne3)
ne3 <- gsub('올해', '', ne3)
ne3 <- gsub('장면', '', ne3)
ne3 <- gsub('남자', '', ne3)
ne3 <- gsub('여자', '', ne3)
ne3 <- gsub('만큼', '', ne3)
ne3 <- gsub('니가', '', ne3)
ne3 <- gsub('년대', '', ne3)
ne3 <- gsub('옆사람', '', ne3)
ne3 <- gsub('들이', '', ne3)
ne3 <- gsub('저녁', '', ne3)
write(unlist(ne3), 'lala_negative.txt')
ne4 <- read.table('lala_negative.txt')
ne_wordcount <- table(ne4)
# 5. 긍정 단어와 부정단어를 각각 워드 클라우드로 그려서 한 화면에 출력한다.
graphics.off()
palete <- brewer.pal(9,'Set1')
par(new=T, mfrow=c(1,2))
wordcloud(names(po_wordcount), freq=po_wordcount, scale=c(3,1), rot.per=0.1, random.order = F,
random.color = T, col=rainbow(15))
title(main='라라랜드의 긍정적인 평가', col.main='blue')
wordcloud(names(ne_wordcount), freq=ne_wordcount, scale=c(3,1), rot.per=0.1, random.order = F,
random.color = T, col=rainbow(15))
title(main='라라랜드의 부정적인 평가', col.main='red')
library(tm)
library(stringr)
library(arules)
lala <- read.csv('라라랜드.csv', header=T, stringsAsFactors = F)
lala_positive <- lala[lala$score>=9,c('content')]
lala_positive <- sapply(lala_positive, extractNoun, USE.NAMES=F)
head(lala_positive)
c <- unlist(lala_positive)
lala_positive2 <- Filter(function(x) { nchar(x) >= 2 & nchar(x) <= 5 }  , c)
#4. 데이터 정제작업( 분석하기에 너무 많이 나오는 단어를 삭제하는 작업 )
# 숫자제거
lala_positive2 <- gsub('\\d+','',lala_positive2)
lala_positive2 <- gsub('관람객','',lala_positive2)
lala_positive2 <- gsub('평점', '', lala_positive2)
lala_positive2 <- gsub('영화', '', lala_positive2)
lala_positive2 <- gsub('진짜', '', lala_positive2)
lala_positive2 <- gsub('완전', '', lala_positive2)
lala_positive2 <- gsub('시간', '', lala_positive2)
lala_positive2 <- gsub('올해', '', lala_positive2)
lala_positive2 <- gsub('장면', '', lala_positive2)
lala_positive2 <- gsub('남자', '', lala_positive2)
lala_positive2 <- gsub('여자', '', lala_positive2)
lala_positive2 <- gsub('만큼', '', lala_positive2)
lala_positive2 <- gsub('니가', '', lala_positive2)
lala_positive2 <- gsub('년대', '', lala_positive2)
lala_positive2 <- gsub('옆사람', '', lala_positive2)
lala_positive2 <- gsub('들이', '', lala_positive2)
lala_positive2 <- gsub('저녁', '', lala_positive2)
lala_positive2 <- gsub('영화', '', lala_positive2)
lala_positive2
#4. 데이터 정제작업( 분석하기에 너무 많이 나오는 단어를 삭제하는 작업 )
# 숫자제거
txt=readLines("lalagsub.txt")
cnt=length(txt)
i=1
for(i in 1:cnt){
lala_positive2=gsub((txt[i]),"",lala_positive2)
}
lala_positive2
res <- str_replace_all(lala_positive2, "[^[:alpha:]]","")
res <- res[res != ""]
wordcount <- table(res)
wordcount2 <- sort( table(res), decreasing=T)
keyword <- names( wordcount2[wordcount2>100] )
length(lala_positive)
length(lala_positive2)
#9. 아프리오리 분석을 위해서 표형태로 만드는 작업
contents <- c()
for(i in 1:length(lala_positive2)) {
inter <- intersect(lala_positive2[[i]] , keyword)
contents <- rbind(contents ,table(inter)[keyword])
}
colnames(contents) <- keyword
contents[which(is.na(contents))] <- 0
dim(lala_positive)
detach(package:tm, unload=T)
library(arules)
rules_lala <- apriori(contents , parameter = list(supp = 0.007 , conf = 0.3 , target = "rules"))
rules_lala
inspect(sort(rules_lala))
b2 <- t(as.matrix(contents)) %*% as.matrix(contents)
b2
b2.w <- b2 - diag(diag(b2))
b2.w
#rownames(b2.w)
#colnames(b2.w)
dev.new()
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "pink" ,
edge.col="light green" , boxed.labels=F ,
arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
par(new=T, mfrow=c(1,2))
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "pink" ,
edge.col="light green" , boxed.labels=F ,
arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "pink" ,
edge.col="light green" , boxed.labels=F ,
arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "pink" ,
edge.col="light green" , boxed.labels=F ,
arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*2)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*1.5)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "pink" ,
edge.col="light green" , boxed.labels=F ,
arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*1.5)
lala_positive2
lala <- read.csv('라라랜드.csv', header=T, stringsAsFactors = F)
lala_positive <- lala[lala$score>=9,c('content')]
lala_positive <- sapply(lala_positive, extractNoun, USE.NAMES=F)
c <- unlist(lala_positive)
lala_positive2 <- Filter(function(x){ nchar(x) >= 2 & nchar(x) <= 5 } , c)
lala_positive2 <- gsub('\\d+','',lala_positive2)
txt <- readLines("lalagsub.txt")
cnt <- length(txt)
txt
txt <- readLines("lalagsub.txt")
cnt <- length(txt)
txt
i=1
for(i in 1:cnt){
lala_positive2 <- gsub((txt[i]),'',lala_positive2)
}
lala_positive2
txt <- readLines("lalagsub.txt")
cnt <- length(txt)
txt
i=1
for(i in 1:cnt){
lala_positive2 <- gsub((txt[i]),'',lala_positive2)
}
lala_positive2
lala_positive2 <- Filter(function(x){ nchar(x) >= 2 & nchar(x) <= 5 } , c)
lala_positive2
lala_positive2 <- gsub('\\d+','',lala_positive2)
lala_positive2
res <- str_replace_all(lala_positive2, "[^[:alpha:]]","")
res <- res[res != ""]
res
wordcount <- table(res)
wordcount2 <- sort( table(res), decreasing=T)
keyword <- names( wordcount2[wordcount2>100] )
keyword
length(res)
contents <- c()
for(i in 1:length(res)) {
inter <- intersect(res[[i]] , keyword)
contents <- rbind(contents ,table(inter)[keyword])
}
colnames(contents) <- keyword
contents[which(is.na(contents))] <- 0
dim(lala_positive)
detach(package:tm, unload=T)
library(arules)
rules_lala <- apriori(contents , parameter = list(supp = 0.007 , conf = 0.3 , target = "rules"))
rules_lala
inspect(sort(rules_lala))
b2 <- t(as.matrix(contents)) %*% as.matrix(contents)
b2
b2.w <- b2 - diag(diag(b2))
b2.w
#rownames(b2.w)
#colnames(b2.w)
dev.new()
<<<<<<< HEAD
rpart.plot(rpartmod)
savePlot('dcs_tree.png', type='png')
getwd()
# 1. 버섯 데이터를 R 로 로드한다.
mushroom <- read.csv("mushrooms.csv",head=T, stringsAsFactors=T)
str(mushroom)
head(mushroom)
set.seed(11)
dim(mushroom)
train_cnt <- round( 0.75 * dim(mushroom)[1])
length(train_cnt)
nrow(train_cnt)
train_cnt
train_index
mushroom_train <- mushroom[train_index,  ]
mushroom_test  <- mushroom[-train_index, ]
install.packages("OneR")
library(OneR)
model1 <- OneR(type~. ,  data=mushroom_train)
model1
summary(model1)
# 4. 위에서 생성한 모델을 가지고 테스트 데이터로 결과를 확인한다.
result1 <- predict( model1, mushroom_test[   , -1] )
library(gmodels)
CrossTable( mushroom_test[ , 1],  result1)
install.packages("gmodels")
library(gmodels)
CrossTable( mushroom_test[ , 1],  result1)
buy <- data.frame(
cust_name=c('SCOTT','SMITH','ALLEN','JONES','WARD'),
card_yn=c('Y','Y','N','Y','Y'),
intro_yn=c('Y','Y','N','N','Y'),
before_buy_yn=c('Y','Y','Y','N','Y'),
buy_yn=c('Y','Y','N','Y','Y') )
buy
prop.table( table(buy$buy_yn)  )
str(buy)
library(C50)
buy <- buy[,-1]
buy
buy_model <- C5.0( buy[-4] , buy[  , 4] )
buy <- data.frame(
cust_name=c('SCOTT','SMITH','ALLEN','JONES','WARD'),
card_yn=as.factor(c('Y','Y','N','Y','Y')),
intro_yn=as.factor(c('Y','Y','N','N','Y')),
before_buy_yn=as.factor(c('Y','Y','Y','N','Y')),
buy_yn=as.factor(c('Y','Y','N','Y','Y')) )
buy <- buy[,-1]
buy_model <- C5.0( buy[-4] , buy[  , 4] )
buy_model
summary(buy_model)
skin <- read.csv('skin.csv',header=T, stringsAsFactors = T)
str(skin)
head(skin)
install.packages('FSelector')
library(FSelector)
weights <- information.gain(cupon_react~., skin, unit='log2')
print(weights)
fat <- read.csv('fatliver2.csv',head=T,stringsAsFactors = T)
str(fat)
rs <- information.gain(FATLIVER~., fat, unit='log2')
print(rs)
install.packages("C50")
library(C50)
skin <- read.csv("skin.csv", head=T ,stringsAsFactors = T)
skin_real_test_cust <- skin[30,  ]  # 나중에 모델 만들고 정확도를 올린후에 최종적으로 모델이 잘 맞추는지 확인하려 한건 제외 시킨다.
skin2 <-  skin[ 1:29, ]
skin_real_test_cust
skin2 <- skin2[ , -1] # 고객번호를 제외시킨다.
set.seed(20)
skin2_shuffle <- skin2[sample(nrow(skin2)),    ]  # shuffle 시킴
train_num <-  round(0.7 * nrow(skin2_shuffle), 0)
skin2_train <- skin2_shuffle[1:train_num,  ]
skin2_test  <- skin2_shuffle[(train_num+1) : nrow(skin2_shuffle), ]
library(C50)
skin_model <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react )
install.packages("C50")
skin <- read.csv("skin.csv", head=T ,stringsAsFactors = T)
skin_real_test_cust <- skin[30,  ]  # 나중에 모델 만들고 정확도를 올린후에 최종적으로 모델이 잘 맞추는지 확인하려 한건 제외 시킨다.
skin2 <-  skin[ 1:29, ]
skin_real_test_cust
skin2 <- skin2[ , -1] # 고객번호를 제외시킨다.
set.seed(20)
skin2_shuffle <- skin2[sample(nrow(skin2)),    ]  # shuffle 시킴
train_num <-  round(0.7 * nrow(skin2_shuffle), 0)
skin2_train <- skin2_shuffle[1:train_num,  ]
skin2_test  <- skin2_shuffle[(train_num+1) : nrow(skin2_shuffle), ]
library(C50)
skin_model <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react )
skin_model
summary(skin_model)
skin2_result  <- predict( skin_model , skin2_test[  , -6])
CrossTable( skin2_test[  , 6],  skin2_result )
skin_model2 <- C5.0(skin2_train[  , -6],  skin2_train$cupon_react,trials=10 )
skin2_result  <- predict( skin_model2 , skin2_test[  , -6])
CrossTable( skin2_test[  , 6],  skin2_result )
print(rs)
buy
buy <- data.frame(
cust_name=c('SCOTT','SMITH','ALLEN','JONES','WARD'),
card_yn=c('Y','Y','N','Y','Y'),
intro_yn=c('Y','Y','N','N','Y'),
before_buy_yn=c('Y','Y','Y','N','Y'),
buy_yn=c('Y','Y','N','Y','Y') )
rs <- information.gain(buy_yn~., buy, unit='log2')
print(rs)
# 1. 데이터를 로드한다.
reg <- read.table("regression.txt", header=T)
reg
#  2. 데이터를 시각화 한다.
attach(reg)
plot(growth~tannin, data = reg, pch=21, col='blue', bg='red')
# 3. 회귀분석을 해서 회귀 계수인 기울기와 절편을 구하시오 !
m <- lm( growth ~ tannin, data=reg)
#             ↑     ↑           ↑
#       회귀함수  종속변수  독립변수
m
# 4. 위의 산포도 그래프에 회귀 직선을 그린다.
abline( m ,  col='red')
# 5. 그래프 제목을 회귀 직선의 방정식으로 출력되게한다.
title(paste( '성장률=', round(m$coefficients[2], 4), "* 탄닌 + ", round(m$coefficients[1], 4)))
# 6. 위의 그래프에 잔차를 그린다.
y_hat <- predict( m, tannin=tannin)
y_hat
reg
y_hat
join <- function(i){
lines( c(tannin[i], tannin[i]), c( growth[i],y_hat[i]), col="green")
}
sapply(1:9, join)
plot(growth~tannin, data = reg, pch=21, col='blue', bg='red')
# 3. 회귀분석을 해서 회귀 계수인 기울기와 절편을 구하시오 !
m <- lm( growth ~ tannin, data=reg)
#    ↑       ↑        ↑
#  reg함수  종속    독립
m
# 4. 위의 산포도 그래프에 회귀 직선을 그린다.
abline( m ,  col='red')
sapply(1:9, join) #
# 1. 데이터를 로드한다.
adv <- read.csv("simple_hg.csv", header=T)
adv
str(adv)
#  2. 데이터를 시각화 한다.
attach(adv)
plot(input~cost, data = adv, pch=21, col='blue', bg='red')
# 3. 회귀분석을 해서 회귀 계수인 기울기와 절편을 구하시오 !
adv_m <- lm( input ~ cost, data=adv)
#    ↑       ↑        ↑
#  reg함수  종속    독립
adv_m # 회귀계수-> 직선의 기울기
# 4. 위의 산포도 그래프에 회귀 직선을 그린다.
abline( m ,  col='red')
# 4. 위의 산포도 그래프에 회귀 직선을 그린다.
abline( adv_m ,  col='red')
# 5. 그래프 제목을 회귀 직선의 방정식으로 출력되게한다.
title(paste( '증가율=', round(adv_m$coefficients[2], 4), "*광고비+ ", round(m$coefficients[1], 4)))
# 5. 그래프 제목을 회귀 직선의 방정식으로 출력되게한다.
title(paste( '증가율=', round(adv_m$coefficients[2], 4), "*광고비+ ", round(adv_m$coefficients[1], 4)))
#    ↑       ↑        ↑
#  reg함수  종속    독립
adv_m # 회귀계수-> 직선의 기울기
# 4. 위의 산포도 그래프에 회귀 직선을 그린다.
abline( adv_m ,  col='red')
# 5. 그래프 제목을 회귀 직선의 방정식으로 출력되게한다.
title(paste( '증가율=', round(adv_m$coefficients[2], 4), "*광고비+ ", round(adv_m$coefficients[1], 4)))
plot(input~cost, data = adv, pch=21, col='blue', bg='red')
# 4. 위의 산포도 그래프에 회귀 직선을 그린다.
abline( adv_m ,  col='red')
# 5. 그래프 제목을 회귀 직선의 방정식으로 출력되게한다.
title(paste( '증가율=', round(adv_m$coefficients[2], 4), "*광고비+ ", round(adv_m$coefficients[1], 4)))
# 6. 위의 그래프에 잔차를 그린다.
y_hat <- predict( adv_m, cost=cost) #
y_hat
join <- function(i){
lines( c(cost[i], cost[i]), c( input[i],y_hat[i]), col="green")
}
sapply(1:9, join) #
sapply(1:nrow(adv), join) #
# 1. 데이터를 로드한다.
launch <- read.csv("challenger.csv")
launch
# 2. lm 회귀함수로 기울기와 절편을 구한다.
attach(launch)
plot( distress_ct ~ temperature,  data=launch,  col="red", bg="red", pch=21)
m <- lm( distress_ct ~ temperature, launch)
abline( m ,  col="blue")
names(launch)
plot(distress_ct ~ temperature, data=launch)
m <- lm(distress_ct  ~  temperature, launch)
plot( distress_ct ~ temperature,  data=launch,  col="red", bg="red", pch=21)
m <- lm( distress_ct ~ temperature, launch)
abline( m ,  col="blue")
abline(m, col='red')
abline( m ,  col="blue")
title(paste('파손수=', round(m$coefficients[1], 4), "* 온도 + ",
round(m$coefficients[2], 4)))
cor(launch$temperature,launch$distress_ct)
k_index <- read.csv("K_index.csv", header=T,  stringsAsFactors=F)
s_stock <- read.csv("S_stock.csv", header=T, stringsAsFactors=F)
h_stock <- read.csv("H_stock.csv", header=T,  stringsAsFactors=F)
all_data <- merge(merge(k_index,s_stock), h_stock)
head(all_data)
attach(all_data)
#             y축(삼성 수익율 등락 비율)
#               ↓
plot(k_rate, s_rate, col="blue")
plot(k_rate, h_rate, col="blue")
par(mfrow=c(1,3))
#             y축(삼성 수익율 등락 비율)
#               ↓
plot(k_rate, s_rate, col="blue")
plot(k_rate, h_rate, col="blue")
model_s <- lm( s_rate ~ k_rate, data=all_data)
abline( model_s,  col="red")
plot(k_rate, h_rate, col="blue")
model_h <- lm( h_rate ~ k_rate, data=all_data)
abline( model_h,  col="red")
graphics.off()
par(mfrow=c(1,2), new=T)
par(mar=c(2,2,2,2) )
plot(k_rate, s_rate, col="blue")
model_s <- lm( s_rate ~ k_rate, data=all_data)
abline( model_s,  col="red")
plot(k_rate, h_rate, col="blue")
model_h <- lm( h_rate ~ k_rate, data=all_data)
abline( model_h,  col="red")
all_data
#             y축(삼성 수익율 등락 비율)
#               ↓
plot(s_rate, k_rate, col="blue")
plot(h_rate, k_rate, col="blue")
model_s <- lm( k_rate ~ s_rate, data=all_data)
abline( model_s,  col="red")
#plot(k_rate, s_rate, col="blue")
plot(s_rate, k_rate, col="blue") # s to kos
model_s <- lm( k_rate ~ s_rate, data=all_data) # s affect to kos
abline( model_s,  col="red")
#plot(k_rate, h_rate, col="blue")
plot(h_rate, k_rate, col="blue") # h to kos
model_h <- lm( k_rate ~ h_rate, data=all_data) # h aff to kos
abline( model_h,  col="red")
cor(all_data$k_rate,all_data$s_rate)
cor(all_data$k_rate, all_data$s_rate)
cov(all_data$k_rate, all_data$s_rate)
all_data
head(all_data)
attach(all_data)
cov(all_data$k_rate, all_data$s_rate)
install.packages('shiny')
library(shiny)
runExample("01_hello")
runExample("03_reactivity")
ui <- fluidPage(
fluidRow(
column(5, "test")
)
)
server <- function(input, output, session) {
}
shinyApp(ui, server)
shiny::runGitHub('GDAA', 'hyunyulhenry')
install.packages('tibble')
install.packages("tibble")
install.packages('tibble')
install.packages("tibble")
shiny::runGitHub('GDAA', 'hyunyulhenry')
install.packages('tibble')
shiny::runGitHub('GDAA', 'hyunyulhenry')
library(tibble)
mj <- read.csv('mw_job.csv',head=T)
View(mj)
=======
par(new=T, mfrow=c(1,2))
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*1.5)
gplot(b2.w , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "pink" ,
edge.col="light green" , boxed.labels=F ,
arrowhead.cex = .3 , label.pos = 3 , edge.lwd = b2.w*1.5)
>>>>>>> 0a0c2ac25db7f0907364d1d7736865daa27f0e0f
